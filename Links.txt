https://www.ncbi.nlm.nih.gov/pmc/articles/PMC4867776/
Predicting possible mutations in RNA strands


https://www.sciencedirect.com/topics/computer-science/sigmoid-function
Sigmoid use explained

https://datascience.stackexchange.com/questions/30676/role-derivative-of-sigmoid-function-in-neural-networks
The use of derivatives in neural networks is for the training process called backpropagation. This technique uses gradient descent in order to find an optimal set of model parameters in order to minimize a loss function. In your example you must use the derivative of a sigmoid because that is the activation that your individual neurons are using.

The loss function
The essence of machine learning is to optimize a cost function such that we can either minimize or maximize some target function. This is typically called the loss or cost function. We typically want to minimize this function. The cost function, ùê∂, associates some penalty based on the resulting errors when passing data through your model as a function of the model parameters.


https://ai.stackexchange.com/questions/5546/what-is-the-difference-between-a-convolutional-neural-network-and-a-regular-neur/


https://www.machinecurve.com/index.php/2020/12/01/how-to-check-if-your-deep-learning-model-is-underfitting-or-overfitting/

Using loss and visualization to see if model is overfitting / underfitting the data given


https://machinelearningmastery.com/how-to-configure-the-number-of-layers-and-nodes-in-a-neural-network/
Why multiple layers

https://towardsdatascience.com/machine-learning-fundamentals-via-linear-regression-41a5d11f5220
Something about Lin reg


--------
Ideas
--------
1. Using XYZLineChart to visualize how the neural network optimizes across different training samples
2. ReLU Implementation to stop Vanishing Gradient issue with multiple layers

IMPLEMENTATION
Average Daily Rate - https://towardsdatascience.com/regression-based-neural-networks-with-tensorflow-v2-0-predicting-average-daily-rates-e20fffa7ac9a

Diabetes - https://machinelearningmastery.com/tutorial-first-neural-network-python-keras/

Predict House Prices - https://www.freecodecamp.org/news/how-to-build-your-first-neural-network-to-predict-house-prices-with-keras-f8db83049159/


https://stats.stackexchange.com/questions/126238/what-are-the-advantages-of-relu-over-sigmoid-function-in-deep-neural-networks
The main reason why ReLu is used is because it is simple, fast, and empirically it seems to work well.

Empirically, early papers observed that training a deep network with ReLu tended to converge much more quickly and reliably than training a deep network with sigmoid activation. In the early days, people were able to train deep networks with ReLu but training deep networks with sigmoid flat-out failed. There are many hypotheses that have attempted to explain why this could be.

First, with a standard sigmoid activation, the gradient of the sigmoid is typically some fraction between 0 and 1; if you have many layers, these multiply, and might give an overall gradient that is exponentially small, so each step of gradient descent will make only a tiny change to the weights, leading to slow convergence (the vanishing gradient problem). In contrast, with ReLu activation, the gradient of the ReLu is either 0 or 1, so after many layers often the gradient will include the product of a bunch of 1's, and thus the overall gradient is not too small or not too large. But this story might be too simplistic, because it doesn't take into account the way that we multiply by the weights and add up internal activations.

Second, with sigmoid activation, the gradient goes to zero if the input is very large or very small. When the gradient goes to zero, gradient descent tends to have very slow convergence. In contrast, with ReLu activation, the gradient goes to zero if the input is negative but not if the input is large, so it might have only "half" of the problems of sigmoid. But this seems a bit naive too as it is clear that negative values still give a zero gradient.

Since then, we've accumulated more experience and more tricks that can be used to train neural networks. For instance, batch normalization is very helpful. When you add in those tricks, the comparison becomes less clear. It is possible to successfully train a deep network with either sigmoid or ReLu, if you apply the right set of tricks.

I suspect that ultimately there are several reasons for widespread use of ReLu today:

Historical accident: we discovered ReLu in the early days before we knew about those tricks, so in the early days ReLu was the only choice that worked, and everyone had to use it. And now that everyone uses it, it is a safe choice and people keep using it.

Efficiency: ReLu is faster to compute than the sigmoid function, and its derivative is faster to compute. This makes a significant difference to training and inference time for neural networks: only a constant factor, but constants can matter.

Simplicity: ReLu is simple.

Fragility: empirically, ReLu seems to be a bit more forgiving (in terms of the tricks needed to make the network train successfully), whereas sigmoid is more fiddly (to train a deep network, you need more tricks, and it's more fragile).

Good enough: empirically, in many domains, other activation functions are no better than ReLu, or if they are better, are better by only a tiny amount. So, if ReLu is simple, fast, and about as good as anything else in most settings, it makes a reasonable default.


--------
User Manual (Feed-forward neural network)
--------

1. Adjusting Layers/Neurons
*	Make sure the amount of input neurons is the same as the amount of input variables in the input
*	Adjusting hidden layers to more than 1 with a small set of training data will result in inaccurate network training
*	Adding neurons into each hidden layer will result in longer run-time

2. Manually inputting Training Data
*	Training Data must correspond with amount of input neurons or else program will crash
*	Input must look like below

v1,v2,v3,vn... : o1
v1,v2,v3,vn... : o2
v1,v2,v3,vn... : o3
v1,v2,v3,vn... : on

Ex:
0,0,0:0
0,0,1:0
0,1,0:0
1,0,0:0
1,1,0:0
1,0,1:1
0,1,1:1
1,1,1:1



